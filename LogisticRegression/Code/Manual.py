# -*- coding: utf-8 -*-
"""Tarea3.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OnoSjsaH5Tydjtr1cOscyGv0IaGSJ3sj

# Adecuación del metodo manual de regresión Logística

1. Implementar (adecuar) el mètodo "manual" descrito en: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#id13. Con los datos en el csv de clasificación en teams en la carpeta semana 6 .​ Puede descargar el código también en el github del autor, pero por favor leer primero en su totalidad el contenido en el link de arriba. el github del autor es este: https://github.com/bfortuner/ml-glossary
"""

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
import pandas as pd
import matplotlib 
import matplotlib.pyplot as plt
import seaborn as sb
import numpy as np


data= pd.read_csv('data_classification.csv', sep = ';')
data.head()
X = data[['suenio','estudio']].values
y = data['pasan'].values

X = X.T
y = y.reshape(1, X.shape[1])

print("Shape of X : ", X.shape)
print("Shape of Y : ", y.shape)

def sigmoid(x):
    # Activation function used to map any real value between 0 and 1
    return 1 / (1 + np.exp(-x))

def model(X, Y, learning_rate, iterations):
    
    m = X.shape[1]
    n = X.shape[0]
    
    W = np.zeros((n,1))
    B = 0
    
    cost_list = []
    
    for i in range(iterations):
        
        Z = np.dot(W.T, X) + B
        A = sigmoid(Z)
        
        # cost function
        cost = -(1/m)*np.sum( Y*np.log(A) + (1-Y)*np.log(1-A))
        
        # Gradient Descent
        dW = (1/m)*np.dot(A-Y, X.T)
        dB = (1/m)*np.sum(A - Y)
        
        W = W - learning_rate*dW.T
        B = B - learning_rate*dB
        
        # Keeping track of our cost function value
        cost_list.append(cost)
        
        if(i%(iterations/10) == 0):
            print("cost after ", i, "iteration is : ", cost)
        
    return W, B, cost_list

iterations = 100000
learning_rate = 0.0015
W, B, cost_list = model(X, y, learning_rate, iterations)

plt.plot(np.arange(iterations), cost_list)
plt.show()

def accuracy(X, Y, W, B):
    
    Z = np.dot(W.T, X) + B
    A = sigmoid(Z)
    
    A = A > 0.5
    
    A = np.array(A, dtype = 'int64')
    
    acc = (1 - np.sum(np.absolute(A - Y))/Y.shape[1])*100
    
    print("Accuracy of the model is : ", round(acc, 2), "%")

accuracy(X, y, W, B)